{
    "normalization": "quantile",
    "model": {
        "d_layers": [
            119,
            135,
            135,
            92
        ],
        "dropout": 0.08809227719075471
    },
    "training": {
        "eval_batch_size": 8192,
        "lr": 0.001814930917194763,
        "lr_n_decays": 0,
        "optimizer": "adamw",
        "patience": 16,
        "weight_decay": 0.0001458699387750375
    }
}